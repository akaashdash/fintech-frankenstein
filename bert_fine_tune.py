# -*- coding: utf-8 -*-
"""BERT Fine Tune

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NxCPy6b2QxTd9gPUL7KBDx5nvtcr88iM

##Make sure you are using a GPU Runtime

Install dependencies
"""

!pip install transformers pandas scikit-learn datasets tqdm

"""Imports"""

import torch
import pandas as pd
import json
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.utils import shuffle
from datasets import load_from_disk
from tqdm import tqdm

"""Connect to drive

Note: Before running this, add the VIP folder as a shortcut to your drive

*Borrowed this from 4644 lol*
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %load_ext autoreload
# %autoreload 2

import os
#change to desired path
os.chdir("./drive/MyDrive/VIP/router")
# %pwd

"""Datasets list (from Dataset Downloader)"""

flare_datasets = [
    "ChanceFocus/flare-fpb",
    "ChanceFocus/flare-fiqasa",
    "ChanceFocus/flare-headlines",
    "ChanceFocus/flare-finqa",
    "ChanceFocus/flare-convfinqa",
    "ChanceFocus/flare-finer-ord",
    "ChanceFocus/flare-sm-bigdata",
    "ChanceFocus/flare-sm-acl",
    "ChanceFocus/flare-sm-cikm",
    "ChanceFocus/flare-german",
    "ChanceFocus/flare-australian",
    "ChanceFocus/flare-edtsum",
    "ChanceFocus/flare-fomc",
    "ChanceFocus/flare-ner"
]

"""Load datasets

*tqdm idea/usage borrowed from 4664*
"""

loaded_datasets = {}
with tqdm(flare_datasets, position = 0, desc = 'Dataset') as tdqm_datasets:
  for flare_dataset in tdqm_datasets:
      loaded_datasets[flare_dataset] = load_from_disk(f"./data/{flare_dataset}")

all_data = []
for dataset_name, dataset in loaded_datasets.items():
    for split in dataset.keys():
        if split in ['train', 'validate', 'test']:
            df = pd.DataFrame(dataset[split]['query'], columns=['query'])
            df['label'] = dataset_name
            all_data.append(df)

combined_df = pd.concat(all_data)

"""Create label dictionary"""

label_unique = combined_df['label'].unique()
label_dict = {label: index for index, label in enumerate(label_unique)}
combined_df['label'] = combined_df['label'].map(label_dict)

with open('label_dict.json', 'w') as ld_file:
    json.dump(label_dict, ld_file)

"""Shuffle the dataframe

This ensures that locally related rows arent kept together in training/validation to create invalid splits
"""

combined_df = combined_df.sample(frac=1).reset_index(drop=True)

"""Get pretrained BERT and setup preprocessing framework"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

class TextDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        text = str(self.dataframe.iloc[idx]['query'])
        label = int(self.dataframe.iloc[idx]['label'])

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            return_attention_mask=True,
            truncation=True
        )

        return {
            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),
            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),
            'labels': torch.tensor(label, dtype=torch.long)
        }

"""Hyperparameters"""

MAX_LEN = 256
BATCH_SIZE = 32
NUM_EPOCHS = 1
LEARNING_RATE = 1e-7

"""Create data split"""

train_df, val_df = train_test_split(combined_df, test_size=0.2)
train_dataset = TextDataset(train_df, tokenizer, MAX_LEN)
val_dataset = TextDataset(val_df, tokenizer, MAX_LEN)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

"""Train

*tqdm idea/usage borrowed from 4664*
"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_unique))
model = model.to(device)

optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

def evaluate(model, val_loader):
    model.eval()
    val_accuracy = []
    val_precision = []
    val_recall = []

    with tqdm(val_loader, position = 0, desc = 'Test Batch') as tdqm_val_loader:
      for batch in tdqm_val_loader:
          input_ids = batch['input_ids'].to(device)
          attention_mask = batch['attention_mask'].to(device)
          labels = batch['labels'].to(device)

          with torch.no_grad():
              outputs = model(input_ids, attention_mask=attention_mask)

          logits = outputs.logits
          predictions = torch.argmax(logits, dim=-1)
          metrics = precision_recall_fscore_support(labels.cpu(), predictions.cpu(), average='micro')[:3]

          val_accuracy.append(accuracy_score(labels.cpu(), predictions.cpu()))
          val_precision.append(metrics[0])
          val_recall.append(metrics[1])

    avg_acc = sum(val_accuracy) / len(val_accuracy)
    avg_precision = sum(val_precision) / len(val_precision)
    avg_recall = sum(val_recall) / len(val_recall)
    return avg_acc, avg_precision, avg_recall

with tqdm(range(NUM_EPOCHS), position = 1, desc = 'Training Progress') as tdqm_epochs:
  for epoch in tdqm_epochs:
      model.train()
      with tqdm(train_loader, position = 0, desc = 'Train Batch') as tdqm_train_loader:
        for batch in tdqm_train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

      acc, precision, recall = evaluate(model, val_loader)
      print(f"Epoch {epoch + 1} - Accuracy: {acc}, Precision: {precision}, Recall: {recall}")

"""Save model"""

model.save_pretrained("./model")
tokenizer.save_pretrained("./model")

"""Quick predict - change text at the bottom

Meant to test right after training, for full routing use other notebook
"""

def predict(text, model, tokenizer, max_len):
    model.eval()
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_len,
        return_token_type_ids=False,
        padding='max_length',
        return_attention_mask=True,
        truncation=True,
        return_tensors='pt'
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)

    logits = outputs.logits
    prediction = torch.argmax(logits, dim=-1).item()
    for label, index in label_dict.items():
        if index == prediction:
            return label

predicted_label = predict("Evaluate the sentiment of the following text: Foxconn to Invest $1.5 Billion to Expand India Operations", model, tokenizer, MAX_LEN)
print(f"Predicted label: {predicted_label}")