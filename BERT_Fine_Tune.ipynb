{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMCXy71P3dpY"
      },
      "source": [
        "##Make sure you are using a GPU Runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYkcYcg73qpU"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsz18BRi3sJg",
        "outputId": "f503a4b5-8b79-4384-d121-308ead7eafaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers pandas scikit-learn datasets tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kDyW1mJ3uci"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8A1XIuz3veZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.utils import shuffle\n",
        "from datasets import load_from_disk\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWog-3n24EKH"
      },
      "source": [
        "Connect to drive\n",
        "\n",
        "Note: Before running this, add the VIP folder as a shortcut to your drive\n",
        "\n",
        "*Borrowed this from 4644 lol*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "63rZeOzf4FFS",
        "outputId": "4f9a025f-5cfe-4d68-8593-443e02625fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/.shortcut-targets-by-id/1UiU6uLtX7kvXdtuDnHtarlFc7ijUrMO6/VIP/router'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "#change to desired path\n",
        "os.chdir(\"./drive/MyDrive/VIP/router\")\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSwWso6f5CE-"
      },
      "source": [
        "Datasets list (from Dataset Downloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIDka1nK_sqo"
      },
      "outputs": [],
      "source": [
        "flare_datasets = [\n",
        "    \"ChanceFocus/flare-fpb\",\n",
        "    \"ChanceFocus/flare-fiqasa\",\n",
        "    \"ChanceFocus/flare-headlines\",\n",
        "    \"ChanceFocus/flare-finqa\",\n",
        "    \"ChanceFocus/flare-convfinqa\",\n",
        "    \"ChanceFocus/flare-finer-ord\",\n",
        "    \"ChanceFocus/flare-sm-bigdata\",\n",
        "    \"ChanceFocus/flare-sm-acl\",\n",
        "    \"ChanceFocus/flare-sm-cikm\",\n",
        "    \"ChanceFocus/flare-german\",\n",
        "    \"ChanceFocus/flare-australian\",\n",
        "    \"ChanceFocus/flare-edtsum\",\n",
        "    \"ChanceFocus/flare-fomc\",\n",
        "    \"ChanceFocus/flare-ner\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-GMBJsB_p8N"
      },
      "source": [
        "Load datasets\n",
        "\n",
        "*tqdm idea/usage borrowed from 4664*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiM4WN9N_1Ev",
        "outputId": "d981ff67-a0aa-4435-b76f-5a5d516e1dec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dataset: 100%|██████████| 14/14 [01:21<00:00,  5.79s/it]\n"
          ]
        }
      ],
      "source": [
        "loaded_datasets = {}\n",
        "with tqdm(flare_datasets, position = 0, desc = 'Dataset') as tdqm_datasets:\n",
        "  for flare_dataset in tdqm_datasets:\n",
        "      loaded_datasets[flare_dataset] = load_from_disk(f\"./data/{flare_dataset}\")\n",
        "\n",
        "all_data = []\n",
        "for dataset_name, dataset in loaded_datasets.items():\n",
        "    for split in dataset.keys():\n",
        "        if split in ['train', 'validate', 'test']:\n",
        "            df = pd.DataFrame(dataset[split]['query'], columns=['query'])\n",
        "            df['label'] = dataset_name\n",
        "            all_data.append(df)\n",
        "\n",
        "combined_df = pd.concat(all_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6rNfaVID8O3"
      },
      "source": [
        "Create label dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mewtCg63EAq9"
      },
      "outputs": [],
      "source": [
        "label_unique = combined_df['label'].unique()\n",
        "label_dict = {label: index for index, label in enumerate(label_unique)}\n",
        "combined_df['label'] = combined_df['label'].map(label_dict)\n",
        "\n",
        "with open('label_dict.json', 'w') as ld_file:\n",
        "    json.dump(label_dict, ld_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9kF9GQwPUmn"
      },
      "source": [
        "Shuffle the dataframe\n",
        "\n",
        "This ensures that locally related rows arent kept together in training/validation to create invalid splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zfYWBjSdPby0"
      },
      "outputs": [],
      "source": [
        "combined_df = combined_df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtirI59Z5RZL"
      },
      "source": [
        "Get pretrained BERT and setup preprocessing framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "1ad90adab9ee4917b6a017a8b3c9ab95",
            "e75ebbef42be4d00927c3bef873d0d0d",
            "e1b71b3c64ba4d98bb78ee83161176cc",
            "961c3b4f4e264db48e1a311866188b5e"
          ]
        },
        "id": "EXAJPDIu8d7I",
        "outputId": "e8a63685-820c-407e-d726-f0630a786836"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ad90adab9ee4917b6a017a8b3c9ab95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e75ebbef42be4d00927c3bef873d0d0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1b71b3c64ba4d98bb78ee83161176cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "961c3b4f4e264db48e1a311866188b5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.dataframe.iloc[idx]['query'])\n",
        "        label = int(self.dataframe.iloc[idx]['label'])\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgSLFakv9wNQ"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_YG46I-U9438"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 1e-7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNfhkr1E8qFB"
      },
      "source": [
        "Create data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QGbNiXqs8sGp"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(combined_df, test_size=0.2)\n",
        "train_dataset = TextDataset(train_df, tokenizer, MAX_LEN)\n",
        "val_dataset = TextDataset(val_df, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA2gAWGX8vEG"
      },
      "source": [
        "Train\n",
        "\n",
        "*tqdm idea/usage borrowed from 4664*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "bed5dad56cd24c9e9385933b73db939e"
          ]
        },
        "id": "UZOWzD7G8w8l",
        "outputId": "e5dd10a8-a63a-4059-e432-a1801d757d67"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bed5dad56cd24c9e9385933b73db939e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\n",
            "Train Batch: 100%|██████████| 3907/3907 [40:00<00:00,  1.63it/s]\n",
            "Test Batch: 100%|██████████| 977/977 [05:45<00:00,  2.83it/s]\n",
            "\n",
            "Training Progress: 100%|██████████| 1/1 [45:46<00:00, 2746.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Accuracy: 0.8598601160013648, Precision: 0.8598601160013648, Recall: 0.8598601160013648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_unique))\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "\n",
        "    with tqdm(val_loader, position = 0, desc = 'Test Batch') as tdqm_val_loader:\n",
        "      for batch in tdqm_val_loader:\n",
        "          input_ids = batch['input_ids'].to(device)\n",
        "          attention_mask = batch['attention_mask'].to(device)\n",
        "          labels = batch['labels'].to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "          logits = outputs.logits\n",
        "          predictions = torch.argmax(logits, dim=-1)\n",
        "          metrics = precision_recall_fscore_support(labels.cpu(), predictions.cpu(), average='micro')[:3]\n",
        "\n",
        "          val_accuracy.append(accuracy_score(labels.cpu(), predictions.cpu()))\n",
        "          val_precision.append(metrics[0])\n",
        "          val_recall.append(metrics[1])\n",
        "\n",
        "    avg_acc = sum(val_accuracy) / len(val_accuracy)\n",
        "    avg_precision = sum(val_precision) / len(val_precision)\n",
        "    avg_recall = sum(val_recall) / len(val_recall)\n",
        "    return avg_acc, avg_precision, avg_recall\n",
        "\n",
        "with tqdm(range(NUM_EPOCHS), position = 1, desc = 'Training Progress') as tdqm_epochs:\n",
        "  for epoch in tdqm_epochs:\n",
        "      model.train()\n",
        "      with tqdm(train_loader, position = 0, desc = 'Train Batch') as tdqm_train_loader:\n",
        "        for batch in tdqm_train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "      acc, precision, recall = evaluate(model, val_loader)\n",
        "      print(f\"Epoch {epoch + 1} - Accuracy: {acc}, Precision: {precision}, Recall: {recall}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VxatvLpAaqX"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fujVMQhAAbUO",
        "outputId": "1918a5ce-ca02-4b02-8bd0-a9d7d5a6976f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./model/tokenizer_config.json',\n",
              " './model/special_tokens_map.json',\n",
              " './model/vocab.txt',\n",
              " './model/added_tokens.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"./model\")\n",
        "tokenizer.save_pretrained(\"./model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j89S83g69PTX"
      },
      "source": [
        "Quick predict - change text at the bottom\n",
        "\n",
        "Meant to test right after training, for full routing use other notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoYhZhtq9R42"
      },
      "outputs": [],
      "source": [
        "def predict(text, model, tokenizer, max_len):\n",
        "    model.eval()\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=-1).item()\n",
        "    for label, index in label_dict.items():\n",
        "        if index == prediction:\n",
        "            return label\n",
        "\n",
        "predicted_label = predict(\"Evaluate the sentiment of the following text: Foxconn to Invest $1.5 Billion to Expand India Operations\", model, tokenizer, MAX_LEN)\n",
        "print(f\"Predicted label: {predicted_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyRkE0DuTRct"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}